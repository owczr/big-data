{"cells":[{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from pyspark.context import SparkContext\n","from pyspark.sql.session import SparkSession\n","sc = SparkContext('local')\n","spark = SparkSession(sc)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Actors"]},{"cell_type":"code","execution_count":22,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8c3328b1-5f50-4d1c-aeb5-b7a1bf9bd93c","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["DataFrame[imdb_title_id: string, ordering: int, imdb_name_id: string, category: string, job: string, characters: string]"]},"metadata":{},"output_type":"display_data"}],"source":["# filepath = \"dbfs:/FileStore/tables/Files/actors.csv\"\n","filepath = \"actors.csv\"\n","df = spark.read.format(\"csv\") \\\n","    .option(\"inferSchema\", \"True\") \\\n","    .option(\"header\", \"True\") \\\n","    .load(filepath)\n","\n","display(df)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"134e7de0-ad69-4e17-b891-1fcbb6b2ec12","showTitle":false,"title":""}},"source":["**See current schema**"]},{"cell_type":"code","execution_count":23,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"412ea576-458a-4ff4-868b-e0a987b0e1da","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- imdb_title_id: string (nullable = true)\n"," |-- ordering: integer (nullable = true)\n"," |-- imdb_name_id: string (nullable = true)\n"," |-- category: string (nullable = true)\n"," |-- job: string (nullable = true)\n"," |-- characters: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"00bd95c4-7d0e-41b0-af00-82320323984c","showTitle":false,"title":""}},"source":["**Create new schema**"]},{"cell_type":"code","execution_count":24,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"11539b62-eb70-432d-990f-27942bf98c35","showTitle":false,"title":""}},"outputs":[],"source":["schema = \"title STRING, order INT, name STRING, category STRING, job STRING, characters STRING\""]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fe93c84a-cd7c-46bd-b254-ccae60611fea","showTitle":false,"title":""}},"source":["**Read DataFrame with new schema**"]},{"cell_type":"code","execution_count":25,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2e83029d-f11f-4291-941e-dd8efcc4442a","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["DataFrame[title: string, order: int, name: string, category: string, job: string, characters: string]"]},"metadata":{},"output_type":"display_data"}],"source":["df = spark.read.format(\"csv\") \\\n","    .schema(schema) \\\n","    .option(\"header\", \"True\") \\\n","    .load(filepath)\n","\n","display(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bbea3751-e87b-40bf-802d-28811a99e12b","showTitle":false,"title":""}},"source":["#### Ex. 3\n","**Create json by hand**"]},{"cell_type":"code","execution_count":34,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bd8197f5-7314-41d1-9a7d-6ab027c53478","showTitle":false,"title":""}},"outputs":[],"source":["json = '{\"title\": \"tt0000009\",\"order\": 1,\"name\": \"nm0063086\",\"category\": \"actress\",\"job\": \"null\",\"characters\": \"[Miss Geraldine Holbrook (Miss Jerry)]\"}'\n","df_json = spark.read.json(sc.parallelize([json]))"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+--------------------+----+---------+-----+---------+\n","|category|          characters| job|     name|order|    title|\n","+--------+--------------------+----+---------+-----+---------+\n","| actress|[Miss Geraldine H...|null|nm0063086|    1|tt0000009|\n","+--------+--------------------+----+---------+-----+---------+\n","\n"]}],"source":["df_json.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"45e394e4-2c5a-42fd-a77b-86d4b99a4f7f","showTitle":false,"title":""}},"source":["**or write DataFrame to json and read again**"]},{"cell_type":"code","execution_count":36,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"22dff8b4-8c28-4954-be73-f5056d11403b","showTitle":false,"title":""}},"outputs":[],"source":["# df.select(\"title\", \"name\").write.save(\"title_name.json\", format=\"json\")"]},{"cell_type":"code","execution_count":37,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5683087c-6fa8-4eb8-923e-769f604fa0da","showTitle":false,"title":""}},"outputs":[],"source":["# df_json = spark.read.format(\"json\").json(\"title_name.json\")\n","\n","# df_json.show()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"12f65ed0-9fb7-483c-9e57-8d5b11d874b3","showTitle":false,"title":""}},"source":["#### Ex. 4\n","**Damaging the data**"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["json = '\"title\": \"tt0000009\",\"order\" 1,\"name\": \"nm0063086\",\"category\": \"actress\" \"job\": \"null\",\"characters\": \"[Miss Geraldine Holbrook (Miss Jerry)]\"}'"]},{"cell_type":"code","execution_count":53,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a85f771b-6152-4d87-b8d5-7e419e3cd8a3","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+-----+----+--------+----+----------+\n","|title|order|name|category| job|characters|\n","+-----+-----+----+--------+----+----------+\n","| null| null|null|    null|null|      null|\n","+-----+-----+----+--------+----+----------+\n","\n"]}],"source":["df_1 = spark.read.schema(schema) \\\n","    .option(\"header\", \"True\") \\\n","    .option(\"badRecordsPath\", \"/mnt/sources/badrecords\") \\\n","    .json(sc.parallelize([json]))\n","\n","df_1.show()"]},{"cell_type":"code","execution_count":54,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4427929a-56f6-4370-b186-484cdc72aaa5","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+-----+----+--------+----+----------+\n","|title|order|name|category| job|characters|\n","+-----+-----+----+--------+----+----------+\n","| null| null|null|    null|null|      null|\n","+-----+-----+----+--------+----+----------+\n","\n"]}],"source":["df_2 = spark.read.schema(schema) \\\n","    .option(\"header\", \"True\") \\\n","    .option(\"mode\", \"PERMISSIVE\") \\\n","    .json(sc.parallelize([json]))\n","\n","df_2.show()"]},{"cell_type":"code","execution_count":55,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9a438136-d9a5-49c6-92e3-a4d1c6b67870","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+-----+----+--------+---+----------+\n","|title|order|name|category|job|characters|\n","+-----+-----+----+--------+---+----------+\n","+-----+-----+----+--------+---+----------+\n","\n"]}],"source":["df_3 = spark.read.schema(schema) \\\n","    .option(\"header\", \"True\") \\\n","    .option(\"mode\", \"DROPMALFORMED\") \\\n","    .json(sc.parallelize([json]))\n","\n","df_3.show()"]},{"cell_type":"code","execution_count":56,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"989edc42-065a-4182-bd76-b6f0f8291fce","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["23/03/19 17:37:58 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 21)\n","org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n","\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n","\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$12(DataFrameReader.scala:430)\n","\tat org.apache.spark.sql.DataFrameReader$$Lambda$3357/1880042285.apply(Unknown Source)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n","\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2523/1015148357.apply(Unknown Source)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n","\tat org.apache.spark.rdd.RDD$$Lambda$2520/455397224.apply(Unknown Source)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2482/1590036297.apply(Unknown Source)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n","\tat java.lang.Thread.run(Thread.java:745)\n","Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:513)\n","\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:426)\n","\tat org.apache.spark.sql.DataFrameReader$$Lambda$3356/1771540971.apply(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n","\t... 24 more\n","Caused by: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseValueForDataTypeError(QueryExecutionErrors.scala:1159)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:406)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:389)\n","\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:377)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:101)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3105/1482905720.apply(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:501)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3115/284009358.apply(Unknown Source)\n","\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2764)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:496)\n","\t... 27 more\n","23/03/19 17:37:59 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 21) (192.168.0.171 executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n","\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n","\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$12(DataFrameReader.scala:430)\n","\tat org.apache.spark.sql.DataFrameReader$$Lambda$3357/1880042285.apply(Unknown Source)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n","\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2523/1015148357.apply(Unknown Source)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n","\tat org.apache.spark.rdd.RDD$$Lambda$2520/455397224.apply(Unknown Source)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2482/1590036297.apply(Unknown Source)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n","\tat java.lang.Thread.run(Thread.java:745)\n","Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:513)\n","\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:426)\n","\tat org.apache.spark.sql.DataFrameReader$$Lambda$3356/1771540971.apply(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n","\t... 24 more\n","Caused by: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseValueForDataTypeError(QueryExecutionErrors.scala:1159)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:406)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:389)\n","\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:377)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:101)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3105/1482905720.apply(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:501)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3115/284009358.apply(Unknown Source)\n","\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2764)\n","\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:496)\n","\t... 27 more\n","\n","23/03/19 17:37:59 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling o577.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21) (192.168.0.171 executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$12(DataFrameReader.scala:430)\n\tat org.apache.spark.sql.DataFrameReader$$Lambda$3357/1880042285.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2523/1015148357.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD$$Lambda$2520/455397224.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2482/1590036297.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:513)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:426)\n\tat org.apache.spark.sql.DataFrameReader$$Lambda$3356/1771540971.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 24 more\nCaused by: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseValueForDataTypeError(QueryExecutionErrors.scala:1159)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:406)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:389)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:377)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3105/1482905720.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:501)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3115/284009358.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2764)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:496)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3185/1552265644.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3183/1085910077.apply(Unknown Source)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset$$Lambda$1581/547834026.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.Dataset$$Lambda$1915/1535253222.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset$$Lambda$1582/2128262915.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1593/16354643.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1583/1813565250.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$12(DataFrameReader.scala:430)\n\tat org.apache.spark.sql.DataFrameReader$$Lambda$3357/1880042285.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2523/1015148357.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD$$Lambda$2520/455397224.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2482/1590036297.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:513)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:426)\n\tat org.apache.spark.sql.DataFrameReader$$Lambda$3356/1771540971.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 24 more\nCaused by: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseValueForDataTypeError(QueryExecutionErrors.scala:1159)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:406)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:389)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:377)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3105/1482905720.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:501)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3115/284009358.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2764)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:496)\n\t... 27 more\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m df_4 \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mschema(schema) \\\n\u001b[1;32m      2\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFAILFAST\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[39m.\u001b[39mjson(sc\u001b[39m.\u001b[39mparallelize([json]))\n\u001b[0;32m----> 6\u001b[0m df_4\u001b[39m.\u001b[39;49mshow()\n","File \u001b[0;32m~/miniconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m~/miniconda3/envs/bigdata/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n","File \u001b[0;32m~/miniconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/miniconda3/envs/bigdata/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o577.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21) (192.168.0.171 executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$12(DataFrameReader.scala:430)\n\tat org.apache.spark.sql.DataFrameReader$$Lambda$3357/1880042285.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2523/1015148357.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD$$Lambda$2520/455397224.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2482/1590036297.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:513)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:426)\n\tat org.apache.spark.sql.DataFrameReader$$Lambda$3356/1771540971.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 24 more\nCaused by: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseValueForDataTypeError(QueryExecutionErrors.scala:1159)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:406)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:389)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:377)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3105/1482905720.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:501)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3115/284009358.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2764)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:496)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3185/1552265644.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3183/1085910077.apply(Unknown Source)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset$$Lambda$1581/547834026.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.Dataset$$Lambda$1915/1535253222.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset$$Lambda$1582/2128262915.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1593/16354643.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1583/1813565250.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$12(DataFrameReader.scala:430)\n\tat org.apache.spark.sql.DataFrameReader$$Lambda$3357/1880042285.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2523/1015148357.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD$$Lambda$2520/455397224.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2482/1590036297.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:513)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:426)\n\tat org.apache.spark.sql.DataFrameReader$$Lambda$3356/1771540971.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 24 more\nCaused by: java.lang.RuntimeException: Failed to parse field name null, field value title, [VALUE_STRING] to target spark data type [StructType(StructField(title,StringType,true),StructField(order,IntegerType,true),StructField(name,StringType,true),StructField(category,StringType,true),StructField(job,StringType,true),StructField(characters,StringType,true))].\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseValueForDataTypeError(QueryExecutionErrors.scala:1159)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:406)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:389)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:377)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:101)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3105/1482905720.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:501)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$3115/284009358.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2764)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:496)\n\t... 27 more\n"]}],"source":["df_4 = spark.read.schema(schema) \\\n","    .option(\"header\", \"True\") \\\n","    .option(\"mode\", \"FAILFAST\") \\\n","    .json(sc.parallelize([json]))\n","\n","df_4.show()"]},{"cell_type":"code","execution_count":57,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"71e6c83e-5a2f-4256-ba84-f91dc3a57a4c","showTitle":false,"title":""}},"outputs":[],"source":["spark.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Json","notebookOrigID":2519879072093514,"widgets":{}},"kernelspec":{"display_name":"bigdata","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
